{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(1)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Convert the train data into time series\n",
    "def time_series_data(data, window, lag):\n",
    "    dropnan = True\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    for i in range(window, 0, -1):\n",
    "        #past time series data (t-)\n",
    "        cols.append(data.shift(i))\n",
    "        names = names + [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    \n",
    "    #current time series data (t = 0)\n",
    "    cols.append(data)\n",
    "    names = names + [('%s(t)' % (col)) for col in data.columns]\n",
    "    \n",
    "    #future data (t + lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names = names + [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    \n",
    "    #all data\n",
    "    all_data = pd.concat(cols, axis=1)\n",
    "    all_data.columns = names\n",
    "    \n",
    "    #drops rows with NaN\n",
    "    if dropnan:\n",
    "        all_data.dropna(inplace=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Store  Item        Date  Weekly_Sales\n",
      "0          1     1  2010-02-07      24924.50\n",
      "2955       1     1  2010-02-14      46039.49\n",
      "5911       1     1  2010-02-21      41595.55\n",
      "8888       1     1  2010-02-28      19403.54\n",
      "11839      1     1  2010-03-07      21827.90\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('sales data-set.csv', usecols = ['Store', 'Item', 'Date', 'Weekly_Sales'])\n",
    "train = dataset[['Store', 'Item', 'Date', 'Weekly_Sales']]\n",
    "train = train.loc[(train['Store'] == 1) & (train['Item'] == 1)]\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item</th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-07</td>\n",
       "      <td>2.492450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-14</td>\n",
       "      <td>4.603949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-21</td>\n",
       "      <td>4.159555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1.940354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-07</td>\n",
       "      <td>2.182790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item  Store        Date  Weekly_Sales\n",
       "0     1      1  2010-02-07      2.492450\n",
       "1     1      1  2010-02-14      4.603949\n",
       "2     1      1  2010-02-21      4.159555\n",
       "3     1      1  2010-02-28      1.940354\n",
       "4     1      1  2010-03-07      2.182790"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-arranges the train dataset to apply shift methods\n",
    "train_r = train.sort_values('Date').groupby(['Item', 'Store', 'Date'], as_index=False)\n",
    "train_r = train_r.agg({'Weekly_Sales':['mean']})\n",
    "train_r.columns = ['Item', 'Store', 'Date', 'Weekly_Sales']\n",
    "#weekly sales in 10 thousand dollars\n",
    "train_r['Weekly_Sales'] = train_r['Weekly_Sales'] / 10000\n",
    "train_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item(t-117)</th>\n",
       "      <th>Store(t-117)</th>\n",
       "      <th>Date(t-117)</th>\n",
       "      <th>Weekly_Sales(t-117)</th>\n",
       "      <th>Item(t-116)</th>\n",
       "      <th>Store(t-116)</th>\n",
       "      <th>Date(t-116)</th>\n",
       "      <th>Weekly_Sales(t-116)</th>\n",
       "      <th>Item(t-115)</th>\n",
       "      <th>Store(t-115)</th>\n",
       "      <th>...</th>\n",
       "      <th>Date(t-1)</th>\n",
       "      <th>Weekly_Sales(t-1)</th>\n",
       "      <th>Item(t)</th>\n",
       "      <th>Store(t)</th>\n",
       "      <th>Date(t)</th>\n",
       "      <th>Weekly_Sales(t)</th>\n",
       "      <th>Item(t+12)</th>\n",
       "      <th>Store(t+12)</th>\n",
       "      <th>Date(t+12)</th>\n",
       "      <th>Weekly_Sales(t+12)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-07</td>\n",
       "      <td>2.492450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-14</td>\n",
       "      <td>4.603949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-04-29</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-06</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-07-29</td>\n",
       "      <td>1.573118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-14</td>\n",
       "      <td>4.603949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-21</td>\n",
       "      <td>4.159555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-05-06</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-13</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-08-05</td>\n",
       "      <td>1.662831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-21</td>\n",
       "      <td>4.159555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1.940354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-05-13</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-20</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-08-12</td>\n",
       "      <td>1.611992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1.940354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-03-07</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-05-20</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-27</td>\n",
       "      <td>1.696355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>1.733070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-03-07</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-03-14</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-05-27</td>\n",
       "      <td>1.696355</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-06-03</td>\n",
       "      <td>1.606549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>1.628640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Item(t-117)  Store(t-117) Date(t-117)  Weekly_Sales(t-117)  Item(t-116)  \\\n",
       "117          1.0           1.0  2010-02-07             2.492450          1.0   \n",
       "118          1.0           1.0  2010-02-14             4.603949          1.0   \n",
       "119          1.0           1.0  2010-02-21             4.159555          1.0   \n",
       "120          1.0           1.0  2010-02-28             1.940354          1.0   \n",
       "121          1.0           1.0  2010-03-07             2.182790          1.0   \n",
       "\n",
       "     Store(t-116) Date(t-116)  Weekly_Sales(t-116)  Item(t-115)  Store(t-115)  \\\n",
       "117           1.0  2010-02-14             4.603949          1.0           1.0   \n",
       "118           1.0  2010-02-21             4.159555          1.0           1.0   \n",
       "119           1.0  2010-02-28             1.940354          1.0           1.0   \n",
       "120           1.0  2010-03-07             2.182790          1.0           1.0   \n",
       "121           1.0  2010-03-14             2.104339          1.0           1.0   \n",
       "\n",
       "     ...   Date(t-1)  Weekly_Sales(t-1)  Item(t)  Store(t)     Date(t)  \\\n",
       "117  ...  2012-04-29           1.634760        1         1  2012-05-06   \n",
       "118  ...  2012-05-06           1.714744        1         1  2012-05-13   \n",
       "119  ...  2012-05-13           1.816420        1         1  2012-05-20   \n",
       "120  ...  2012-05-20           1.851779        1         1  2012-05-27   \n",
       "121  ...  2012-05-27           1.696355        1         1  2012-06-03   \n",
       "\n",
       "     Weekly_Sales(t)  Item(t+12)  Store(t+12)  Date(t+12)  Weekly_Sales(t+12)  \n",
       "117         1.714744         1.0          1.0  2012-07-29            1.573118  \n",
       "118         1.816420         1.0          1.0  2012-08-05            1.662831  \n",
       "119         1.851779         1.0          1.0  2012-08-12            1.611992  \n",
       "120         1.696355         1.0          1.0  2012-08-19            1.733070  \n",
       "121         1.606549         1.0          1.0  2012-08-26            1.628640  \n",
       "\n",
       "[5 rows x 476 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model will use last 117 weekly sales data and \n",
    "#current timestep (7 days) to forecast next weekly sales data 12 weeks ahead\n",
    "\n",
    "window = 117\n",
    "lag = 12\n",
    "\n",
    "series_data = time_series_data(train_r, window, lag)\n",
    "series_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales(t-117)</th>\n",
       "      <th>Weekly_Sales(t-116)</th>\n",
       "      <th>Weekly_Sales(t-115)</th>\n",
       "      <th>Weekly_Sales(t-114)</th>\n",
       "      <th>Weekly_Sales(t-113)</th>\n",
       "      <th>Weekly_Sales(t-112)</th>\n",
       "      <th>Weekly_Sales(t-111)</th>\n",
       "      <th>Weekly_Sales(t-110)</th>\n",
       "      <th>Weekly_Sales(t-109)</th>\n",
       "      <th>Weekly_Sales(t-108)</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekly_Sales(t-9)</th>\n",
       "      <th>Weekly_Sales(t-8)</th>\n",
       "      <th>Weekly_Sales(t-7)</th>\n",
       "      <th>Weekly_Sales(t-6)</th>\n",
       "      <th>Weekly_Sales(t-5)</th>\n",
       "      <th>Weekly_Sales(t-4)</th>\n",
       "      <th>Weekly_Sales(t-3)</th>\n",
       "      <th>Weekly_Sales(t-2)</th>\n",
       "      <th>Weekly_Sales(t-1)</th>\n",
       "      <th>Weekly_Sales(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2.492450</td>\n",
       "      <td>4.603949</td>\n",
       "      <td>4.159555</td>\n",
       "      <td>1.940354</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>2.213664</td>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>...</td>\n",
       "      <td>2.011303</td>\n",
       "      <td>2.114007</td>\n",
       "      <td>2.236688</td>\n",
       "      <td>2.210770</td>\n",
       "      <td>2.895286</td>\n",
       "      <td>5.759212</td>\n",
       "      <td>3.468421</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4.603949</td>\n",
       "      <td>4.159555</td>\n",
       "      <td>1.940354</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>2.213664</td>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>...</td>\n",
       "      <td>2.114007</td>\n",
       "      <td>2.236688</td>\n",
       "      <td>2.210770</td>\n",
       "      <td>2.895286</td>\n",
       "      <td>5.759212</td>\n",
       "      <td>3.468421</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>4.159555</td>\n",
       "      <td>1.940354</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>2.213664</td>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>1.614535</td>\n",
       "      <td>...</td>\n",
       "      <td>2.236688</td>\n",
       "      <td>2.210770</td>\n",
       "      <td>2.895286</td>\n",
       "      <td>5.759212</td>\n",
       "      <td>3.468421</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.851779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.940354</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>2.213664</td>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>1.614535</td>\n",
       "      <td>1.655511</td>\n",
       "      <td>...</td>\n",
       "      <td>2.210770</td>\n",
       "      <td>2.895286</td>\n",
       "      <td>5.759212</td>\n",
       "      <td>3.468421</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1.696355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2.182790</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>2.213664</td>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>1.614535</td>\n",
       "      <td>1.655511</td>\n",
       "      <td>1.741394</td>\n",
       "      <td>...</td>\n",
       "      <td>2.895286</td>\n",
       "      <td>5.759212</td>\n",
       "      <td>3.468421</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1.696355</td>\n",
       "      <td>1.606549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weekly_Sales(t-117)  Weekly_Sales(t-116)  Weekly_Sales(t-115)  \\\n",
       "117             2.492450             4.603949             4.159555   \n",
       "118             4.603949             4.159555             1.940354   \n",
       "119             4.159555             1.940354             2.182790   \n",
       "120             1.940354             2.182790             2.104339   \n",
       "121             2.182790             2.104339             2.213664   \n",
       "\n",
       "     Weekly_Sales(t-114)  Weekly_Sales(t-113)  Weekly_Sales(t-112)  \\\n",
       "117             1.940354             2.182790             2.104339   \n",
       "118             2.182790             2.104339             2.213664   \n",
       "119             2.104339             2.213664             2.622921   \n",
       "120             2.213664             2.622921             5.725843   \n",
       "121             2.622921             5.725843             4.296091   \n",
       "\n",
       "     Weekly_Sales(t-111)  Weekly_Sales(t-110)  Weekly_Sales(t-109)  \\\n",
       "117             2.213664             2.622921             5.725843   \n",
       "118             2.622921             5.725843             4.296091   \n",
       "119             5.725843             4.296091             1.759696   \n",
       "120             4.296091             1.759696             1.614535   \n",
       "121             1.759696             1.614535             1.655511   \n",
       "\n",
       "     Weekly_Sales(t-108)  ...  Weekly_Sales(t-9)  Weekly_Sales(t-8)  \\\n",
       "117             4.296091  ...           2.011303           2.114007   \n",
       "118             1.759696  ...           2.114007           2.236688   \n",
       "119             1.614535  ...           2.236688           2.210770   \n",
       "120             1.655511  ...           2.210770           2.895286   \n",
       "121             1.741394  ...           2.895286           5.759212   \n",
       "\n",
       "     Weekly_Sales(t-7)  Weekly_Sales(t-6)  Weekly_Sales(t-5)  \\\n",
       "117           2.236688           2.210770           2.895286   \n",
       "118           2.210770           2.895286           5.759212   \n",
       "119           2.895286           5.759212           3.468421   \n",
       "120           5.759212           3.468421           1.697619   \n",
       "121           3.468421           1.697619           1.634760   \n",
       "\n",
       "     Weekly_Sales(t-4)  Weekly_Sales(t-3)  Weekly_Sales(t-2)  \\\n",
       "117           5.759212           3.468421           1.697619   \n",
       "118           3.468421           1.697619           1.634760   \n",
       "119           1.697619           1.634760           1.714744   \n",
       "120           1.634760           1.714744           1.816420   \n",
       "121           1.714744           1.816420           1.851779   \n",
       "\n",
       "     Weekly_Sales(t-1)  Weekly_Sales(t)  \n",
       "117           1.634760         1.714744  \n",
       "118           1.714744         1.816420  \n",
       "119           1.816420         1.851779  \n",
       "120           1.851779         1.696355  \n",
       "121           1.696355         1.606549  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drops last record of (t + lag)\n",
    "last_record_item = 'Item(t-%d)' % window\n",
    "last_record_store = 'Store(t-%d)' % window\n",
    "series_data = series_data[(series_data['Item(t)'] == series_data[last_record_item])]\n",
    "series_data = series_data[(series_data['Store(t)'] == series_data[last_record_store])]\n",
    "\n",
    "#drops Item and Store columns\n",
    "cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Item', 'Store', 'Date']]\n",
    "for i in range(window, 0, -1):\n",
    "    cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Item', 'Store', 'Date']]\n",
    "\n",
    "series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "series_data.drop(['Item(t)', 'Store(t)', 'Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "lbls = series_data[lbls_col]\n",
    "series_data = series_data.drop(lbls_col, axis=1)\n",
    "\n",
    "series_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (9, 118)\n",
      "Test dataset: (5, 118)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales(t-117)</th>\n",
       "      <th>Weekly_Sales(t-116)</th>\n",
       "      <th>Weekly_Sales(t-115)</th>\n",
       "      <th>Weekly_Sales(t-114)</th>\n",
       "      <th>Weekly_Sales(t-113)</th>\n",
       "      <th>Weekly_Sales(t-112)</th>\n",
       "      <th>Weekly_Sales(t-111)</th>\n",
       "      <th>Weekly_Sales(t-110)</th>\n",
       "      <th>Weekly_Sales(t-109)</th>\n",
       "      <th>Weekly_Sales(t-108)</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekly_Sales(t-9)</th>\n",
       "      <th>Weekly_Sales(t-8)</th>\n",
       "      <th>Weekly_Sales(t-7)</th>\n",
       "      <th>Weekly_Sales(t-6)</th>\n",
       "      <th>Weekly_Sales(t-5)</th>\n",
       "      <th>Weekly_Sales(t-4)</th>\n",
       "      <th>Weekly_Sales(t-3)</th>\n",
       "      <th>Weekly_Sales(t-2)</th>\n",
       "      <th>Weekly_Sales(t-1)</th>\n",
       "      <th>Weekly_Sales(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.741394</td>\n",
       "      <td>1.892674</td>\n",
       "      <td>1.477304</td>\n",
       "      <td>1.558043</td>\n",
       "      <td>1.755809</td>\n",
       "      <td>1.663762</td>\n",
       "      <td>1.621627</td>\n",
       "      <td>1.632872</td>\n",
       "      <td>1.633314</td>\n",
       "      <td>1.768876</td>\n",
       "      <td>...</td>\n",
       "      <td>1.606549</td>\n",
       "      <td>1.766600</td>\n",
       "      <td>1.755882</td>\n",
       "      <td>1.663341</td>\n",
       "      <td>1.572282</td>\n",
       "      <td>1.782337</td>\n",
       "      <td>1.656618</td>\n",
       "      <td>1.634806</td>\n",
       "      <td>1.573118</td>\n",
       "      <td>1.662831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>1.614535</td>\n",
       "      <td>1.655511</td>\n",
       "      <td>1.741394</td>\n",
       "      <td>1.892674</td>\n",
       "      <td>1.477304</td>\n",
       "      <td>1.558043</td>\n",
       "      <td>1.755809</td>\n",
       "      <td>1.663762</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1.696355</td>\n",
       "      <td>1.606549</td>\n",
       "      <td>1.766600</td>\n",
       "      <td>1.755882</td>\n",
       "      <td>1.663341</td>\n",
       "      <td>1.572282</td>\n",
       "      <td>1.782337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4.603949</td>\n",
       "      <td>4.159555</td>\n",
       "      <td>1.940354</td>\n",
       "      <td>2.182790</td>\n",
       "      <td>2.104339</td>\n",
       "      <td>2.213664</td>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>...</td>\n",
       "      <td>2.114007</td>\n",
       "      <td>2.236688</td>\n",
       "      <td>2.210770</td>\n",
       "      <td>2.895286</td>\n",
       "      <td>5.759212</td>\n",
       "      <td>3.468421</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2.622921</td>\n",
       "      <td>5.725843</td>\n",
       "      <td>4.296091</td>\n",
       "      <td>1.759696</td>\n",
       "      <td>1.614535</td>\n",
       "      <td>1.655511</td>\n",
       "      <td>1.741394</td>\n",
       "      <td>1.892674</td>\n",
       "      <td>1.477304</td>\n",
       "      <td>1.558043</td>\n",
       "      <td>...</td>\n",
       "      <td>1.697619</td>\n",
       "      <td>1.634760</td>\n",
       "      <td>1.714744</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1.696355</td>\n",
       "      <td>1.606549</td>\n",
       "      <td>1.766600</td>\n",
       "      <td>1.755882</td>\n",
       "      <td>1.663341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1.759696</td>\n",
       "      <td>1.614535</td>\n",
       "      <td>1.655511</td>\n",
       "      <td>1.741394</td>\n",
       "      <td>1.892674</td>\n",
       "      <td>1.477304</td>\n",
       "      <td>1.558043</td>\n",
       "      <td>1.755809</td>\n",
       "      <td>1.663762</td>\n",
       "      <td>1.621627</td>\n",
       "      <td>...</td>\n",
       "      <td>1.816420</td>\n",
       "      <td>1.851779</td>\n",
       "      <td>1.696355</td>\n",
       "      <td>1.606549</td>\n",
       "      <td>1.766600</td>\n",
       "      <td>1.755882</td>\n",
       "      <td>1.663341</td>\n",
       "      <td>1.572282</td>\n",
       "      <td>1.782337</td>\n",
       "      <td>1.656618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weekly_Sales(t-117)  Weekly_Sales(t-116)  Weekly_Sales(t-115)  \\\n",
       "130             1.741394             1.892674             1.477304   \n",
       "126             4.296091             1.759696             1.614535   \n",
       "118             4.603949             4.159555             1.940354   \n",
       "124             2.622921             5.725843             4.296091   \n",
       "127             1.759696             1.614535             1.655511   \n",
       "\n",
       "     Weekly_Sales(t-114)  Weekly_Sales(t-113)  Weekly_Sales(t-112)  \\\n",
       "130             1.558043             1.755809             1.663762   \n",
       "126             1.655511             1.741394             1.892674   \n",
       "118             2.182790             2.104339             2.213664   \n",
       "124             1.759696             1.614535             1.655511   \n",
       "127             1.741394             1.892674             1.477304   \n",
       "\n",
       "     Weekly_Sales(t-111)  Weekly_Sales(t-110)  Weekly_Sales(t-109)  \\\n",
       "130             1.621627             1.632872             1.633314   \n",
       "126             1.477304             1.558043             1.755809   \n",
       "118             2.622921             5.725843             4.296091   \n",
       "124             1.741394             1.892674             1.477304   \n",
       "127             1.558043             1.755809             1.663762   \n",
       "\n",
       "     Weekly_Sales(t-108)  ...  Weekly_Sales(t-9)  Weekly_Sales(t-8)  \\\n",
       "130             1.768876  ...           1.606549           1.766600   \n",
       "126             1.663762  ...           1.714744           1.816420   \n",
       "118             1.759696  ...           2.114007           2.236688   \n",
       "124             1.558043  ...           1.697619           1.634760   \n",
       "127             1.621627  ...           1.816420           1.851779   \n",
       "\n",
       "     Weekly_Sales(t-7)  Weekly_Sales(t-6)  Weekly_Sales(t-5)  \\\n",
       "130           1.755882           1.663341           1.572282   \n",
       "126           1.851779           1.696355           1.606549   \n",
       "118           2.210770           2.895286           5.759212   \n",
       "124           1.714744           1.816420           1.851779   \n",
       "127           1.696355           1.606549           1.766600   \n",
       "\n",
       "     Weekly_Sales(t-4)  Weekly_Sales(t-3)  Weekly_Sales(t-2)  \\\n",
       "130           1.782337           1.656618           1.634806   \n",
       "126           1.766600           1.755882           1.663341   \n",
       "118           3.468421           1.697619           1.634760   \n",
       "124           1.696355           1.606549           1.766600   \n",
       "127           1.755882           1.663341           1.572282   \n",
       "\n",
       "     Weekly_Sales(t-1)  Weekly_Sales(t)  \n",
       "130           1.573118         1.662831  \n",
       "126           1.572282         1.782337  \n",
       "118           1.714744         1.816420  \n",
       "124           1.755882         1.663341  \n",
       "127           1.782337         1.656618  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train and test split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(series_data, lbls.values, test_size=0.3, random_state=0)\n",
    "\n",
    "print('Train dataset:', X_train.shape)\n",
    "print('Test dataset:', X_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "#batch = 32\n",
    "learning_rate = 0.0003\n",
    "adam = optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "387e71579b0a4b5aca85a465c8e1b1a3a3289915"
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 50)                5950      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,001\n",
      "Trainable params: 6,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(50, activation = 'relu', input_dim = X_train.shape[1]))\n",
    "model_mlp.add(Dense(1))\n",
    "model_mlp.compile(loss = 'mse', optimizer=adam)\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 5 samples\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 13.4696 - val_loss: 12.0023\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 11.5982 - val_loss: 9.8688\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 9.6653 - val_loss: 7.9239\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 7.8898 - val_loss: 6.1996\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 333us/step - loss: 6.3114 - val_loss: 4.7436\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 667us/step - loss: 4.9867 - val_loss: 3.5572\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 443us/step - loss: 3.9176 - val_loss: 2.6202\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 441us/step - loss: 3.0805 - val_loss: 1.9141\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 2.4644 - val_loss: 1.4096\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 444us/step - loss: 2.0437 - val_loss: 1.1077\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.7791 - val_loss: 0.9606\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.6517 - val_loss: 0.9260\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 444us/step - loss: 1.6356 - val_loss: 0.9688\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 1.6968 - val_loss: 1.0555\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.7984 - val_loss: 1.1582\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.9125 - val_loss: 1.2518\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 2.0174 - val_loss: 1.3245\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 666us/step - loss: 2.0978 - val_loss: 1.3653\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 778us/step - loss: 2.1417 - val_loss: 1.3696\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 669us/step - loss: 2.1447 - val_loss: 1.3381\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 2.1075 - val_loss: 1.2747\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 445us/step - loss: 2.0333 - val_loss: 1.1867\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 1.9300 - val_loss: 1.0818\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 555us/step - loss: 1.8062 - val_loss: 0.9679\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 889us/step - loss: 1.6709 - val_loss: 0.8538\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 667us/step - loss: 1.5332 - val_loss: 0.7467\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 778us/step - loss: 1.3985 - val_loss: 0.6515\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2715 - val_loss: 0.5728\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 670us/step - loss: 1.1601 - val_loss: 0.5170\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 888us/step - loss: 1.0666 - val_loss: 0.4794\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 667us/step - loss: 0.9925 - val_loss: 0.4577\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.9381 - val_loss: 0.4512\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 889us/step - loss: 0.8991 - val_loss: 0.4555\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 556us/step - loss: 0.8736 - val_loss: 0.4672\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 779us/step - loss: 0.8579 - val_loss: 0.4824\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.8488 - val_loss: 0.4997\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.8430 - val_loss: 0.5143\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 889us/step - loss: 0.8380 - val_loss: 0.5236\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 889us/step - loss: 0.8317 - val_loss: 0.5260\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 667us/step - loss: 0.8228 - val_loss: 0.5209\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 667us/step - loss: 0.8103 - val_loss: 0.5082\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 891us/step - loss: 0.7933 - val_loss: 0.4884\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 667us/step - loss: 0.7721 - val_loss: 0.4626\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 777us/step - loss: 0.7486 - val_loss: 0.4321\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.7222 - val_loss: 0.3986\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6940 - val_loss: 0.3636\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6654 - val_loss: 0.3287\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6373 - val_loss: 0.2951\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.6108 - val_loss: 0.2640\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 886us/step - loss: 0.5866 - val_loss: 0.2374\n"
     ]
    }
   ],
   "source": [
    "mlp = model_mlp.fit(X_train.values, Y_train, validation_data = (X_test.values, Y_test), epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "387e71579b0a4b5aca85a465c8e1b1a3a3289915"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (9, 118, 1)\n",
      "Test dataset: (5, 118, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_series = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print('Train dataset:', X_train_series.shape)\n",
    "print('Test dataset:', X_test_series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 117, 64)           192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 58, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3712)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                185650    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 185,893\n",
      "Trainable params: 185,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters = 64, kernel_size = 2, activation = 'relu', input_shape = (X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(pool_size = 2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(50, activation = 'relu'))\n",
    "model_cnn.add(Dense(1))\n",
    "model_cnn.compile(loss = 'mse', optimizer = adam)\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 5 samples\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 3.5364 - val_loss: 1.3607\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.7007 - val_loss: 0.1425\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2736 - val_loss: 0.5918\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.4220 - val_loss: 1.6167\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 1.2436 - val_loss: 1.5920\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 1.2212 - val_loss: 0.9190\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.3129\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2093 - val_loss: 0.0925\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0940 - val_loss: 0.0917\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1752 - val_loss: 0.1469\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2707 - val_loss: 0.1826\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.3179 - val_loss: 0.1675\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.3010 - val_loss: 0.1189\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2318 - val_loss: 0.0695\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1472 - val_loss: 0.0465\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.0623\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.1103\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0787 - val_loss: 0.1668\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1122 - val_loss: 0.2051\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1376 - val_loss: 0.2098\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1402 - val_loss: 0.1806\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1184 - val_loss: 0.1315\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.0808\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.0433\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.0258\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0237\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0287\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.0328\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0541 - val_loss: 0.0318\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0260\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0390 - val_loss: 0.0189\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0147\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0164\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0230\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.0314\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.0374\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0381\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.0334\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0251\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.0103\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.0076\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0077\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0090\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0098\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0094\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.0078\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.0059\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "cnn = model_cnn.fit(X_train_series, Y_train, validation_data = (X_test_series, Y_test), epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "387e71579b0a4b5aca85a465c8e1b1a3a3289915"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation = 'relu', input_shape = (X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss = 'mse', optimizer = adam)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 5 samples\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 3s 329ms/step - loss: 5.1978 - val_loss: 4.4830\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 5.1246 - val_loss: 4.3930\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 5.0288 - val_loss: 4.2913\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 4.9202 - val_loss: 4.1836\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 4.8050 - val_loss: 4.0736\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 4.6870 - val_loss: 3.9631\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 4.5678 - val_loss: 3.8527\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 4.4494 - val_loss: 3.7424\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 4.3310 - val_loss: 3.6321\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 4.2129 - val_loss: 3.5258\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 4.0998 - val_loss: 3.4255\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 3.9921 - val_loss: 3.3269\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 3.8861 - val_loss: 3.2291\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 3.7810 - val_loss: 3.1321\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 3.6769 - val_loss: 3.0356\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 3.5732 - val_loss: 2.9391\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 3.4699 - val_loss: 2.8431\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 3.3670 - val_loss: 2.7467\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 3.2640 - val_loss: 2.6498\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 3.1606 - val_loss: 2.5523\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 3.0571 - val_loss: 2.4538\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 2.9529 - val_loss: 2.3541\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 2.8472 - val_loss: 2.2531\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 2.7396 - val_loss: 2.1508\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 2.6314 - val_loss: 2.0457\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 2.5204 - val_loss: 1.9384\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 2.4067 - val_loss: 1.8282\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 2.2893 - val_loss: 1.7143\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 2.1683 - val_loss: 1.5944\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 2.0419 - val_loss: 1.4668\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.9085 - val_loss: 1.3291\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.7655 - val_loss: 1.1797\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 1.6105 - val_loss: 1.0145\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4389 - val_loss: 0.8299\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 1.2458 - val_loss: 0.6223\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.0275 - val_loss: 0.3990\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.7865 - val_loss: 0.2868\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.5536 - val_loss: 44.7043\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 69.5283 - val_loss: 0.6273\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.6117 - val_loss: 0.2973\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.5157 - val_loss: 0.2722\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.5980 - val_loss: 0.3197\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.6851 - val_loss: 0.3739\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7533 - val_loss: 0.4278\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.8146 - val_loss: 0.4777\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.8689 - val_loss: 0.5217\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.9161 - val_loss: 0.5611\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.9581 - val_loss: 0.5958\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.9949 - val_loss: 0.6263\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.0270 - val_loss: 0.6531\n"
     ]
    }
   ],
   "source": [
    "lstm = model_lstm.fit(X_train_series, Y_train, validation_data = (X_test_series, Y_test), epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "387e71579b0a4b5aca85a465c8e1b1a3a3289915"
   },
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (9, 2, 59, 1)\n",
      "Test dataset: (5, 2, 59, 1)\n"
     ]
    }
   ],
   "source": [
    "#CNN-LSTM\n",
    "subsequences = 2\n",
    "time_steps = X_train_series.shape[1] // subsequences\n",
    "X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, time_steps, 1))\n",
    "X_test_series_sub = X_test_series.reshape((X_test_series.shape[0], subsequences, time_steps, 1))\n",
    "print('Train dataset:', X_train_series_sub.shape)\n",
    "print('Test dataset:', X_test_series_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_13 (TimeDis (None, None, 59, 64)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, None, 29, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, None, 1856)        0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50)                381400    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 381,579\n",
      "Trainable params: 381,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 1, activation = 'relu'), input_shape = (None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm.add(LSTM(50, activation = 'relu'))\n",
    "model_cnn_lstm.add(Dense(1))\n",
    "model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "model_cnn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 5 samples\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 4s 435ms/step - loss: 3.7103 - val_loss: 0.7420\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 1.0051 - val_loss: 0.3653\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.3805 - val_loss: 1.0216\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9836 - val_loss: 0.3327\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.3257 - val_loss: 0.0624\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.1276 - val_loss: 0.1853\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.3139 - val_loss: 0.2954\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.4507 - val_loss: 0.2755\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.4225 - val_loss: 0.1631\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.2744 - val_loss: 0.0545\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.1123 - val_loss: 0.0556\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0548 - val_loss: 0.1884\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1413 - val_loss: 0.2971\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.2290 - val_loss: 0.2417\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.1793 - val_loss: 0.1079\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0705 - val_loss: 0.0285\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0279\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0546 - val_loss: 0.0559\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.1015 - val_loss: 0.0677\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1180 - val_loss: 0.0513\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0930 - val_loss: 0.0233\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0469 - val_loss: 0.0135\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0398\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0213 - val_loss: 0.0827\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0523 - val_loss: 0.0990\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0662 - val_loss: 0.0724\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0459 - val_loss: 0.0316\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0174 - val_loss: 0.0086\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0213 - val_loss: 0.0151\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0354 - val_loss: 0.0157\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0366 - val_loss: 0.0087\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0242 - val_loss: 0.0034\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0096 - val_loss: 0.0092\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0058 - val_loss: 0.0243\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0351\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0215 - val_loss: 0.0311\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0187 - val_loss: 0.0168\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0090 - val_loss: 0.0049\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0061 - val_loss: 0.0024\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0114 - val_loss: 0.0030\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0016\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0013\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.0118\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0060 - val_loss: 0.0154\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0129\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "cnn_lstm = model_cnn_lstm.fit(X_train_series_sub, Y_train, validation_data = (X_test_series_sub, Y_test), epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "387e71579b0a4b5aca85a465c8e1b1a3a3289915"
   },
   "source": [
    "### Comparison of ANN Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Performances of different ANN models for time series dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#comparing models\n",
    "print(\"              Performances of different ANN models for time series dataset\")\n",
    "fig, axes = plt.subplots(2, 2, sharex = True, sharey = True, figsize = (10, 5))\n",
    "ax1, ax2 = axes[0]\n",
    "ax3, ax4 = axes[1]\n",
    "\n",
    "ax1.plot(mlp.history['loss'], label = 'Training loss', color = 'blue')\n",
    "ax1.plot(mlp.history['val_loss'], label = 'Validation loss', color = 'red')\n",
    "ax1.legend(loc = 'upper left')\n",
    "ax1.set_title('MLP')\n",
    "ax1.set_ylabel('MSE')\n",
    "\n",
    "ax2.plot(cnn.history['loss'], label = 'Training loss', color = 'blue')\n",
    "ax2.plot(cnn.history['val_loss'], label = 'Validation loss', color = 'red')\n",
    "ax2.legend(loc = 'upper left')\n",
    "ax2.set_title('CNN')\n",
    "ax3.plot(lstm.history['loss'], label= 'Training loss', color = 'blue')\n",
    "ax3.plot(lstm.history['val_loss'], label = 'Validation loss', color = 'red')\n",
    "ax3.legend(loc = 'upper left')\n",
    "ax3.set_title('LSTM')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_ylabel('MSE')\n",
    "\n",
    "ax4.plot(cnn_lstm.history['loss'], label = 'Training loss', color = 'blue')\n",
    "ax4.plot(cnn_lstm.history['val_loss'], label = 'Validation loss', color = 'red')\n",
    "ax4.legend(loc = 'upper left')\n",
    "ax4.set_title('CNN-LSTM')\n",
    "ax4.set_xlabel('Epochs')\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('images\\model_comparison.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
