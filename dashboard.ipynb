{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    import PySimpleGUI as sg\n",
    "    import tkinter as Tk\n",
    "else:\n",
    "    import PySimpleGUI27 as sg\n",
    "    import Tkinter as Tk\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasAgg\n",
    "import matplotlib.backends.tkagg as tkagg\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "matplotlib.use('TkAgg')\n",
    "from tkinter import *\n",
    "import tkinter as Tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_empty = 0\n",
    "set_random_seed(1)\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train data into time series\n",
    "def time_series_data(data, window, lag):\n",
    "    dropnan = True\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    for i in range(window, 0, -1):\n",
    "        #past time series data (t-)\n",
    "        cols.append(data.shift(i))\n",
    "        names = names + [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    \n",
    "    #current time series data (t = 0)\n",
    "    cols.append(data)\n",
    "    names = names + [('%s(t)' % (col)) for col in data.columns]\n",
    "    \n",
    "    #future data (t + lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names = names + [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    \n",
    "    #all data\n",
    "    all_data = pd.concat(cols, axis=1)\n",
    "    all_data.columns = names\n",
    "    \n",
    "    #drops rows with NaN\n",
    "    if dropnan:\n",
    "        all_data.dropna(inplace=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_code(item_name):\n",
    "    items = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\demand\\\\items.csv', usecols = ['item_code', 'item_name'])\n",
    "    item_code = items.loc[items['item_name'] == item_name]\n",
    "    item = item_code.iat[0, 0]\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store_loc(store):\n",
    "    stores = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\demand\\\\stores_geo.csv', usecols = ['store_code', 'location', 'geo_code'])\n",
    "    store_loc = stores.loc[stores['store_code'] == store]\n",
    "    location = store_loc.iat[0, 1]\n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = 0\n",
    "item = 0\n",
    "item_name = ''\n",
    "store_loc = ''\n",
    "\n",
    "def get_result_by_item():\n",
    "    try:\n",
    "        \n",
    "        ########################################### sales data ##########################################################\n",
    "        dataset = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\sales data-set.csv', usecols = ['Store', 'Item', 'Date', 'Weekly_Sales'])\n",
    "        train = dataset[['Store', 'Item', 'Date', 'Weekly_Sales']]\n",
    "\n",
    "        train = train.loc[(train['Store'] == store) & (train['Item'] == item)]\n",
    "        div = 10000   #weekly sales in 10 thousand dollars\n",
    "        file_name = \"C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\sales\\\\saved nn weights\\\\by item\\\\cnn_lstm_weights_s\" + str(store) + \"_i\" + str(item) + \".hdf5\"\n",
    "\n",
    "        #Re-arranges the train dataset to apply shift methods\n",
    "        train_r = train.sort_values('Date').groupby(['Item', 'Store', 'Date'], as_index=False)\n",
    "        train_r = train_r.agg({'Weekly_Sales':['mean']})\n",
    "        train_r.columns = ['Item', 'Store', 'Date', 'Weekly_Sales']\n",
    "        #train_r = train_r.groupby(['Date'], as_index=False)['Weekly_Sales'].sum()\n",
    "        train_r['Weekly_Sales'] = train_r['Weekly_Sales'] / div\n",
    "\n",
    "        #the model will use last 117 weekly sales data and \n",
    "        #current timestep (7 days) to forecast next weekly sales data 12 weeks ahead\n",
    "\n",
    "        window = 117\n",
    "        lag = 12\n",
    "\n",
    "        series_data = time_series_data(train_r, window, lag)\n",
    "        future_dates = series_data[['Date(t+%d)' % lag]]\n",
    "\n",
    "        if((store == \"all\") | (store == \"ALL\") | (store == \"All\")):\n",
    "            #drops Item and Store columns\n",
    "            cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "            for i in range(window, 0, -1):\n",
    "                cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "            series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "            series_data.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "            lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "            lbls = series_data[lbls_col]\n",
    "            series_data = series_data.drop(lbls_col, axis=1)\n",
    "        else:\n",
    "            if(item >= 0): \n",
    "                #drops last record of (t + lag)\n",
    "                last_record_item = 'Item(t-%d)' % window\n",
    "                last_record_store = 'Store(t-%d)' % window\n",
    "                series_data = series_data[(series_data['Item(t)'] == series_data[last_record_item])]\n",
    "                series_data = series_data[(series_data['Store(t)'] == series_data[last_record_store])]\n",
    "\n",
    "                #drops Item and Store columns\n",
    "                cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Item', 'Store', 'Date']]\n",
    "                for i in range(window, 0, -1):\n",
    "                    cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Item', 'Store', 'Date']]\n",
    "\n",
    "                series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "                series_data.drop(['Item(t)', 'Store(t)', 'Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "                lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "                lbls = series_data[lbls_col]\n",
    "                series_data = series_data.drop(lbls_col, axis=1)\n",
    "            else:\n",
    "                #drops Item and Store columns\n",
    "                cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "                for i in range(window, 0, -1):\n",
    "                    cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "                series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "                series_data.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "                lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "                lbls = series_data[lbls_col]\n",
    "                series_data = series_data.drop(lbls_col, axis=1)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(series_data, lbls.values, test_size=0.3, random_state=0, shuffle=False)\n",
    "        X_test = series_data     \n",
    "        X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test_series = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        subsequences = 2\n",
    "        time_steps = X_test_series.shape[1] // subsequences\n",
    "        X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, time_steps, 1))\n",
    "        X_test_series_sub = X_test_series.reshape((X_test_series.shape[0], subsequences, time_steps, 1))\n",
    "\n",
    "        epochs = 2000\n",
    "        batch = 32\n",
    "        learning_rate = 0.00000001\n",
    "        adam = optimizers.Adam(learning_rate)\n",
    "\n",
    "        model_cnn_lstm = Sequential()\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 1, activation = 'relu'), input_shape = (None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "        model_cnn_lstm.add(LSTM(60, activation = 'relu'))\n",
    "        model_cnn_lstm.add(Dropout(0.1))\n",
    "        model_cnn_lstm.add(Dense(30, activation = 'relu'))\n",
    "        model_cnn_lstm.add(Dropout(0.1))\n",
    "        model_cnn_lstm.add(Dense(1))\n",
    "        model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "        model_cnn_lstm.load_weights(file_name)\n",
    "        model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "        #prediction\n",
    "        #cnn_lstm_train_prediction = model_cnn_lstm.predict(X_train_series_sub)\n",
    "        cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub)\n",
    "\n",
    "        test_p = []\n",
    "        for i in range(0, cnn_lstm_test_prediction.shape[0]):\n",
    "            test_p.append(cnn_lstm_test_prediction[i][0])\n",
    "\n",
    "        predictions = pd.DataFrame({'Date' : list(future_dates['Date(t+12)']), 'Weekly_Sales' : test_p})\n",
    "\n",
    "        train_p = train_r\n",
    "        actual_data = train_p.tail(117 + 12)\n",
    "        train_p.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "        actual_data.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "\n",
    "        for k in range(0, 12):\n",
    "            X_test_predict = train_p.tail(117 + 12)    \n",
    "            max_date = X_test_predict['Date'].max()\n",
    "            next_date = datetime.datetime.strptime(max_date, '%Y-%m-%d').date() + timedelta(days=7)\n",
    "            df = pd.DataFrame({'Date' : next_date.strftime(\"%Y-%m-%d\"), \n",
    "                               'Weekly_Sales' : [0]})\n",
    "            X_test_predict = X_test_predict.append(df)\n",
    "            train_p = train_p.append(df)\n",
    "            predictions = predictions.append(df)\n",
    "            actual_data = actual_data.append(df)\n",
    "\n",
    "            X_test_predict = X_test_predict.reset_index(drop = True)\n",
    "            train_p = train_p.reset_index(drop = True)\n",
    "            predictions = predictions.reset_index(drop = True)\n",
    "            actual_data = actual_data.reset_index(drop = True)\n",
    "\n",
    "            series_data_p = time_series_data(X_test_predict, window, lag)\n",
    "\n",
    "            cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "            for i in range(window, 0, -1):\n",
    "                cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "            series_data_p.drop(cols_to_drop, axis=1, inplace=True)\n",
    "            series_data_p.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "            lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "            lbls = series_data_p[lbls_col]\n",
    "            series_data_p = series_data_p.drop(lbls_col, axis=1)\n",
    "\n",
    "            X_test_series_p = series_data_p.values.reshape((series_data_p.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "            subsequences = 2\n",
    "            time_steps_p = X_test_series_p.shape[1] // subsequences\n",
    "            X_test_series_sub_p = X_test_series_p.reshape((X_test_series_p.shape[0], subsequences, time_steps_p, 1))   \n",
    "\n",
    "            cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub_p)\n",
    "\n",
    "            train_p.loc[(train_p['Date'] == next_date.strftime(\"%Y-%m-%d\"))  & (train_p['Weekly_Sales'] == 0), ['Weekly_Sales']] = cnn_lstm_test_prediction[0][0]\n",
    "            predictions.loc[(predictions['Date'] == next_date.strftime(\"%Y-%m-%d\")) & (predictions['Weekly_Sales'] == 0), ['Weekly_Sales']] = cnn_lstm_test_prediction[0][0]\n",
    "\n",
    "        graph_data = actual_data.merge(predictions, left_on = ['Date'], right_on = ['Date'], how = 'outer')\n",
    "        graph_data = graph_data.loc[graph_data['Date'] >= '2012-07-29']\n",
    "        graph_data.loc[graph_data['Weekly_Sales_x'] == 0, ['Weekly_Sales_x']] = np.nan #Weekly_Sales_x is actual data and Weekly_Sales_y is the predicted data\n",
    "\n",
    "        plt.subplots(figsize=(15, 6))\n",
    "        plt.subplot(221)\n",
    "        plt.plot(graph_data.Date, graph_data.Weekly_Sales_x, color='blue')\n",
    "        plt.plot(graph_data.Date, graph_data.Weekly_Sales_y, dashes=[10, 5, 10, 5], color='red')\n",
    "        plt.legend(['Actual Sales Data', 'Predicted Sales Data'], loc='upper right')\n",
    "        plt.yticks(np.arange(0, 10, 2))\n",
    "        plt.xticks(graph_data.Date, rotation=90)\n",
    "        plt.title('Weekly sales and forecasts of ' + str(item_name) + ' in Store-' + str(store) + ', Location-' + str(store_loc))\n",
    "        plt.xlabel('Date (yyyy-mm-dd)')\n",
    "        plt.ylabel('Weekly Sales (in 10 million dollars)')\n",
    "        plt.grid(True)\n",
    "\n",
    "        if((store > 0) & (item > 0)):\n",
    "            ########################################### demand data ##########################################################\n",
    "            dataset = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\demand\\\\interest_over_time_s' + str(store) + '.csv', usecols = ['Store', 'Item', 'Date', 'Weekly_Demand'])\n",
    "            train = dataset[['Store', 'Item', 'Date', 'Weekly_Demand']]\n",
    "\n",
    "            train = train.loc[(train['Store'] == store) & (train['Item'] == item)]\n",
    "            div = 10  \n",
    "            file_name = \"C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\demand\\\\saved nn weights\\\\cnn_lstm_gtrends_weights_s\" + str(store) + \"_i\" + str(item) + \".hdf5\"\n",
    "\n",
    "            #Re-arranges the train dataset to apply shift methods\n",
    "            train_r = train.sort_values('Date').groupby(['Item', 'Store', 'Date'], as_index=False)\n",
    "            train_r = train_r.agg({'Weekly_Demand':['mean']})\n",
    "            train_r.columns = ['Item', 'Store', 'Date', 'Weekly_Demand']\n",
    "            #train_r = train_r.groupby(['Date'], as_index=False)['Weekly_Demand'].sum()\n",
    "            train_r['Weekly_Demand'] = train_r['Weekly_Demand'] / div    #weekly demand data in 0-10 scale\n",
    "\n",
    "            #the model will use last 117 weekly sales data and \n",
    "            #current timestep (7 days) to forecast next weekly sales data 12 weeks ahead\n",
    "\n",
    "            window = 117\n",
    "            lag = 12\n",
    "\n",
    "            series_data = time_series_data(train_r, window, lag)\n",
    "            future_dates = series_data[['Date(t+%d)' % lag]]\n",
    "\n",
    "            if (item >= 4): \n",
    "                #drops last record of (t + lag)\n",
    "                last_record_item = 'Item(t-%d)' % window\n",
    "                last_record_store = 'Store(t-%d)' % window\n",
    "                series_data = series_data[(series_data['Item(t)'] == series_data[last_record_item])]\n",
    "                series_data = series_data[(series_data['Store(t)'] == series_data[last_record_store])]\n",
    "\n",
    "                #drops Item and Store columns\n",
    "                cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Item', 'Store', 'Date']]\n",
    "                for i in range(window, 0, -1):\n",
    "                    cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Item', 'Store', 'Date']]\n",
    "\n",
    "                series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "                series_data.drop(['Item(t)', 'Store(t)', 'Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "                lbls_col = 'Weekly_Demand(t+%d)' % lag\n",
    "                lbls = series_data[lbls_col]\n",
    "                series_data = series_data.drop(lbls_col, axis=1)\n",
    "            else:\n",
    "                #drops Item and Store columns\n",
    "                cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "                for i in range(window, 0, -1):\n",
    "                    cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "                series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "                series_data.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "                lbls_col = 'Weekly_Demand(t+%d)' % lag\n",
    "                lbls = series_data[lbls_col]\n",
    "                series_data = series_data.drop(lbls_col, axis=1)\n",
    "\n",
    "\n",
    "            #if(series_data.shape[1] < 143):\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(series_data, lbls.values, test_size=0.3, random_state=0, shuffle=False)\n",
    "            X_test = series_data     \n",
    "            X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "            X_test_series = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "            subsequences = 2\n",
    "            time_steps = X_test_series.shape[1] // subsequences\n",
    "            X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, time_steps, 1))\n",
    "            X_test_series_sub = X_test_series.reshape((X_test_series.shape[0], subsequences, time_steps, 1))\n",
    "\n",
    "            epochs = 5000\n",
    "            batch = 32\n",
    "            learning_rate = 0.00000001\n",
    "            adam = optimizers.Adam(learning_rate)\n",
    "\n",
    "            model_cnn_lstm = Sequential()\n",
    "            model_cnn_lstm.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 1, activation = 'relu'), input_shape = (None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "            model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "            model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "            model_cnn_lstm.add(LSTM(60, activation = 'relu'))\n",
    "            model_cnn_lstm.add(Dropout(0.5))\n",
    "            model_cnn_lstm.add(Dense(30, activation = 'relu'))\n",
    "            model_cnn_lstm.add(Dropout(0.5))\n",
    "            model_cnn_lstm.add(Dense(1))\n",
    "            model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "            model_cnn_lstm.load_weights(file_name)\n",
    "            model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "            #prediction\n",
    "            #cnn_lstm_train_prediction = model_cnn_lstm.predict(X_train_series_sub)\n",
    "            cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub)\n",
    "\n",
    "            test_p = []\n",
    "            for i in range(0, cnn_lstm_test_prediction.shape[0]):\n",
    "                test_p.append(cnn_lstm_test_prediction[i][0])\n",
    "\n",
    "            if (item >= 4): \n",
    "                predictions = pd.DataFrame({#'Item' : 1,\n",
    "                                            #'Store' : 1,\n",
    "                                            'Date' : list(future_dates['Date(t+12)']),\n",
    "                                            'Weekly_Demand' : test_p})\n",
    "            else:\n",
    "                predictions = pd.DataFrame({'Item' : 1,\n",
    "                                            'Store' : 1,\n",
    "                                            'Date' : list(future_dates['Date(t+12)']),\n",
    "                                            'Weekly_Demand' : test_p})\n",
    "\n",
    "            train_p = train_r\n",
    "            actual_data = train_p.tail(117 + 12)\n",
    "            if (item >= 4): \n",
    "                train_p.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "                actual_data.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "\n",
    "            for k in range(0, 12):\n",
    "                X_test_predict = train_p.tail(117 + 12)    \n",
    "                max_date = X_test_predict['Date'].max()\n",
    "                next_date = datetime.datetime.strptime(max_date, '%Y-%m-%d').date() + timedelta(days=7)\n",
    "                if (item >= 4): \n",
    "                    df = pd.DataFrame({#'Item' : 1,\n",
    "                                        #'Store' : 1,\n",
    "                                        'Date' : next_date.strftime(\"%Y-%m-%d\"), \n",
    "                                        'Weekly_Demand' : [0]})\n",
    "                else:\n",
    "                    df = pd.DataFrame({'Item' : 1,\n",
    "                                        'Store' : 1,\n",
    "                                        'Date' : next_date.strftime(\"%Y-%m-%d\"), \n",
    "                                        'Weekly_Demand' : [0]})\n",
    "\n",
    "                X_test_predict = X_test_predict.append(df)\n",
    "                train_p = train_p.append(df)\n",
    "                predictions = predictions.append(df)\n",
    "                actual_data = actual_data.append(df)\n",
    "\n",
    "                X_test_predict = X_test_predict.reset_index(drop = True)\n",
    "                train_p = train_p.reset_index(drop = True)\n",
    "                predictions = predictions.reset_index(drop = True)\n",
    "                actual_data = actual_data.reset_index(drop = True)\n",
    "\n",
    "                series_data_p = time_series_data(X_test_predict, window, lag)\n",
    "\n",
    "                cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "                for i in range(window, 0, -1):\n",
    "                    cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "                series_data_p.drop(cols_to_drop, axis=1, inplace=True)\n",
    "                series_data_p.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "                lbls_col = 'Weekly_Demand(t+%d)' % lag\n",
    "                lbls = series_data_p[lbls_col]\n",
    "                series_data_p = series_data_p.drop(lbls_col, axis=1)\n",
    "\n",
    "                X_test_series_p = series_data_p.values.reshape((series_data_p.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "                subsequences = 2\n",
    "                time_steps_p = X_test_series_p.shape[1] // subsequences\n",
    "                X_test_series_sub_p = X_test_series_p.reshape((X_test_series_p.shape[0], subsequences, time_steps_p, 1))   \n",
    "\n",
    "                cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub_p)\n",
    "\n",
    "                train_p.loc[(train_p['Date'] == next_date.strftime(\"%Y-%m-%d\"))  & (train_p['Weekly_Demand'] == 0), ['Weekly_Demand']] = cnn_lstm_test_prediction[0][0]\n",
    "                predictions.loc[(predictions['Date'] == next_date.strftime(\"%Y-%m-%d\")) & (predictions['Weekly_Demand'] == 0), ['Weekly_Demand']] = cnn_lstm_test_prediction[0][0]\n",
    "\n",
    "            graph_data = actual_data.merge(predictions, left_on = ['Date'], right_on = ['Date'], how = 'outer')\n",
    "            graph_data = graph_data.loc[graph_data['Date'] >= '2012-07-29']\n",
    "            graph_data.loc[graph_data['Weekly_Demand_x'] == 0, ['Weekly_Demand_x']] = np.nan #Weekly_Sales_x is actual data and Weekly_Sales_y is the predicted data\n",
    "\n",
    "            plt.subplot(222)\n",
    "            plt.plot(graph_data.Date, graph_data.Weekly_Demand_x, color='green')\n",
    "            plt.plot(graph_data.Date, graph_data.Weekly_Demand_y, dashes=[10, 5, 10, 5], color='orange')\n",
    "            plt.legend(['Actual Demand Data', 'Predicted Demand Data'], loc='upper right')\n",
    "            plt.yticks(np.arange(0, 10, 2))\n",
    "            plt.xticks(graph_data.Date, rotation=90)\n",
    "            plt.title('Weekly demand and forecasts of ' + str(item_name) + ' in Store-' + str(store) + ', Location-' + str(store_loc))\n",
    "            plt.xlabel('Date (yyyy-mm-dd)')\n",
    "            plt.ylabel('Weekly Demand (in 0-10 scale)')\n",
    "            plt.grid(True)\n",
    "\n",
    "        else:\n",
    "            print(\"Insufficient data!\")\n",
    "    except:\n",
    "        print(\"Insufficient data!\")\n",
    "        \n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_by_store():\n",
    "    try:\n",
    "        \n",
    "        ########################################### sales data ##########################################################\n",
    "        dataset = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\sales data-set.csv', usecols = ['Store', 'Item', 'Date', 'Weekly_Sales'])\n",
    "        train = dataset[['Store', 'Item', 'Date', 'Weekly_Sales']]    \n",
    "        train = train.loc[train['Store'] == store]\n",
    "        div = 1000000   #weekly sales in 10 million dollars\n",
    "        file_name = \"C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\sales\\\\saved nn weights\\\\by store\\\\cnn_lstm_weights_store_\" + str(store) + \".hdf5\"\n",
    "\n",
    "        #Re-arranges the train dataset to apply shift methods\n",
    "        train_r = train.sort_values('Date').groupby(['Item', 'Store', 'Date'], as_index=False)\n",
    "        train_r = train_r.agg({'Weekly_Sales':['mean']})\n",
    "        train_r.columns = ['Item', 'Store', 'Date', 'Weekly_Sales']\n",
    "        train_r = train_r.groupby(['Date'], as_index=False)['Weekly_Sales'].sum()\n",
    "        train_r['Weekly_Sales'] = train_r['Weekly_Sales'] / div\n",
    "\n",
    "        #the model will use last 117 weekly sales data and \n",
    "        #current timestep (7 days) to forecast next weekly sales data 12 weeks ahead\n",
    "\n",
    "        window = 117\n",
    "        lag = 12\n",
    "\n",
    "        series_data = time_series_data(train_r, window, lag)\n",
    "        future_dates = series_data[['Date(t+%d)' % lag]]\n",
    "\n",
    "        #drops Item and Store columns\n",
    "        cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "        for i in range(window, 0, -1):\n",
    "            cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "        series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "        series_data.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "        lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "        lbls = series_data[lbls_col]\n",
    "        series_data = series_data.drop(lbls_col, axis=1)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(series_data, lbls.values, test_size=0.3, random_state=0, shuffle=False)\n",
    "        X_test = series_data     \n",
    "        X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test_series = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        subsequences = 2\n",
    "        time_steps = X_test_series.shape[1] // subsequences\n",
    "        X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, time_steps, 1))\n",
    "        X_test_series_sub = X_test_series.reshape((X_test_series.shape[0], subsequences, time_steps, 1))\n",
    "\n",
    "        epochs = 2000\n",
    "        batch = 32\n",
    "        learning_rate = 0.00000001\n",
    "        adam = optimizers.Adam(learning_rate)\n",
    "\n",
    "        model_cnn_lstm = Sequential()\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 1, activation = 'relu'), input_shape = (None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "        model_cnn_lstm.add(LSTM(60, activation = 'relu'))\n",
    "        model_cnn_lstm.add(Dropout(0.1))\n",
    "        model_cnn_lstm.add(Dense(30, activation = 'relu'))\n",
    "        model_cnn_lstm.add(Dropout(0.1))\n",
    "        model_cnn_lstm.add(Dense(1))\n",
    "        model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "        model_cnn_lstm.load_weights(file_name)\n",
    "        model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "        #prediction\n",
    "        #cnn_lstm_train_prediction = model_cnn_lstm.predict(X_train_series_sub)\n",
    "        cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub)\n",
    "\n",
    "        test_p = []\n",
    "        for i in range(0, cnn_lstm_test_prediction.shape[0]):\n",
    "            test_p.append(cnn_lstm_test_prediction[i][0])\n",
    "\n",
    "        predictions = pd.DataFrame({'Date' : list(future_dates['Date(t+12)']), 'Weekly_Sales' : test_p})\n",
    "\n",
    "        train_p = train_r\n",
    "        actual_data = train_p.tail(117 + 12)\n",
    "        #train_p.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "        #actual_data.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "\n",
    "        for k in range(0, 12):\n",
    "            X_test_predict = train_p.tail(117 + 12)    \n",
    "            max_date = X_test_predict['Date'].max()\n",
    "            next_date = datetime.datetime.strptime(max_date, '%Y-%m-%d').date() + timedelta(days=7)\n",
    "            df = pd.DataFrame({'Date' : next_date.strftime(\"%Y-%m-%d\"), \n",
    "                               'Weekly_Sales' : [0]})\n",
    "            X_test_predict = X_test_predict.append(df)\n",
    "            train_p = train_p.append(df)\n",
    "            predictions = predictions.append(df)\n",
    "            actual_data = actual_data.append(df)\n",
    "\n",
    "            X_test_predict = X_test_predict.reset_index(drop = True)\n",
    "            train_p = train_p.reset_index(drop = True)\n",
    "            predictions = predictions.reset_index(drop = True)\n",
    "            actual_data = actual_data.reset_index(drop = True)\n",
    "\n",
    "            series_data_p = time_series_data(X_test_predict, window, lag)\n",
    "\n",
    "            cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "            for i in range(window, 0, -1):\n",
    "                cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "            series_data_p.drop(cols_to_drop, axis=1, inplace=True)\n",
    "            series_data_p.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "            lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "            lbls = series_data_p[lbls_col]\n",
    "            series_data_p = series_data_p.drop(lbls_col, axis=1)\n",
    "\n",
    "            X_test_series_p = series_data_p.values.reshape((series_data_p.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "            subsequences = 2\n",
    "            time_steps_p = X_test_series_p.shape[1] // subsequences\n",
    "            X_test_series_sub_p = X_test_series_p.reshape((X_test_series_p.shape[0], subsequences, time_steps_p, 1))   \n",
    "\n",
    "            cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub_p)\n",
    "\n",
    "            train_p.loc[(train_p['Date'] == next_date.strftime(\"%Y-%m-%d\"))  & (train_p['Weekly_Sales'] == 0), ['Weekly_Sales']] = cnn_lstm_test_prediction[0][0]\n",
    "            predictions.loc[(predictions['Date'] == next_date.strftime(\"%Y-%m-%d\")) & (predictions['Weekly_Sales'] == 0), ['Weekly_Sales']] = cnn_lstm_test_prediction[0][0]\n",
    "\n",
    "        graph_data = actual_data.merge(predictions, left_on = ['Date'], right_on = ['Date'], how = 'outer')\n",
    "        graph_data = graph_data.loc[graph_data['Date'] >= '2012-07-29']\n",
    "        graph_data.loc[graph_data['Weekly_Sales_x'] == 0, ['Weekly_Sales_x']] = np.nan #Weekly_Sales_x is actual data and Weekly_Sales_y is the predicted data\n",
    "\n",
    "        plt.subplots(figsize=(15, 6))\n",
    "        plt.subplot(221)\n",
    "        plt.plot(graph_data.Date, graph_data.Weekly_Sales_x, color='blue')\n",
    "        plt.plot(graph_data.Date, graph_data.Weekly_Sales_y, dashes=[10, 5, 10, 5], color='red')\n",
    "        plt.legend(['Actual Sales Data', 'Predicted Sales Data'], loc='upper right')\n",
    "        plt.yticks(np.arange(0, 10, 2))\n",
    "        plt.xticks(graph_data.Date, rotation=90)\n",
    "        plt.title('Weekly sales and forecasts of Store-' + str(store) + ', Location-' + str(store_loc))\n",
    "        plt.xlabel('Date (yyyy-mm-dd)')\n",
    "        plt.ylabel('Weekly Sales (in 10 million dollars)')\n",
    "        plt.grid(True)\n",
    "    except:\n",
    "        print(\"Insufficient data!\")\n",
    "\n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_all():\n",
    "    try:\n",
    "        ########################################### sales data ##########################################################\n",
    "        dataset = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\sales data-set.csv', usecols = ['Store', 'Item', 'Date', 'Weekly_Sales'])\n",
    "        train = dataset[['Store', 'Item', 'Date', 'Weekly_Sales']]    \n",
    "        #train = train.loc[train['Store'] == store]\n",
    "        div = 10000000   #weekly sales in 100 million dollars\n",
    "        file_name = \"C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\sales\\\\saved nn weights\\\\by store\\\\cnn_lstm_weights_store_all.hdf5\"\n",
    "\n",
    "        #Re-arranges the train dataset to apply shift methods\n",
    "        train_r = train.sort_values('Date').groupby(['Item', 'Store', 'Date'], as_index=False)\n",
    "        train_r = train_r.agg({'Weekly_Sales':['mean']})\n",
    "        train_r.columns = ['Item', 'Store', 'Date', 'Weekly_Sales']\n",
    "        train_r = train_r.groupby(['Date'], as_index=False)['Weekly_Sales'].sum()\n",
    "        train_r['Weekly_Sales'] = train_r['Weekly_Sales'] / div\n",
    "\n",
    "        #the model will use last 117 weekly sales data and \n",
    "        #current timestep (7 days) to forecast next weekly sales data 12 weeks ahead\n",
    "\n",
    "        window = 117\n",
    "        lag = 12\n",
    "\n",
    "        series_data = time_series_data(train_r, window, lag)\n",
    "        future_dates = series_data[['Date(t+%d)' % lag]]\n",
    "\n",
    "        #drops Item and Store columns\n",
    "        cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "        for i in range(window, 0, -1):\n",
    "            cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "        series_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "        series_data.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "        lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "        lbls = series_data[lbls_col]\n",
    "        series_data = series_data.drop(lbls_col, axis=1)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(series_data, lbls.values, test_size=0.3, random_state=0, shuffle=False)\n",
    "        X_test = series_data     \n",
    "        X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test_series = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        subsequences = 2\n",
    "        time_steps = X_test_series.shape[1] // subsequences\n",
    "        X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, time_steps, 1))\n",
    "        X_test_series_sub = X_test_series.reshape((X_test_series.shape[0], subsequences, time_steps, 1))\n",
    "\n",
    "        epochs = 2000\n",
    "        batch = 32\n",
    "        learning_rate = 0.00000001\n",
    "        adam = optimizers.Adam(learning_rate)\n",
    "\n",
    "        model_cnn_lstm = Sequential()\n",
    "        model_cnn_lstm.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 1, activation = 'relu'), input_shape = (None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "        model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "        model_cnn_lstm.add(LSTM(60, activation = 'relu'))\n",
    "        model_cnn_lstm.add(Dropout(0.1))\n",
    "        model_cnn_lstm.add(Dense(30, activation = 'relu'))\n",
    "        model_cnn_lstm.add(Dropout(0.1))\n",
    "        model_cnn_lstm.add(Dense(1))\n",
    "        model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "        model_cnn_lstm.load_weights(file_name)\n",
    "        model_cnn_lstm.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "        #prediction\n",
    "        #cnn_lstm_train_prediction = model_cnn_lstm.predict(X_train_series_sub)\n",
    "        cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub)\n",
    "\n",
    "        test_p = []\n",
    "        for i in range(0, cnn_lstm_test_prediction.shape[0]):\n",
    "            test_p.append(cnn_lstm_test_prediction[i][0])\n",
    "\n",
    "        predictions = pd.DataFrame({'Date' : list(future_dates['Date(t+12)']), 'Weekly_Sales' : test_p})\n",
    "\n",
    "        train_p = train_r\n",
    "        actual_data = train_p.tail(117 + 12)\n",
    "        #train_p.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "        #actual_data.drop(['Item', 'Store'], axis=1, inplace=True)\n",
    "\n",
    "        for k in range(0, 12):\n",
    "            X_test_predict = train_p.tail(117 + 12)    \n",
    "            max_date = X_test_predict['Date'].max()\n",
    "            next_date = datetime.datetime.strptime(max_date, '%Y-%m-%d').date() + timedelta(days=7)\n",
    "            df = pd.DataFrame({'Date' : next_date.strftime(\"%Y-%m-%d\"), \n",
    "                               'Weekly_Sales' : [0]})\n",
    "            X_test_predict = X_test_predict.append(df)\n",
    "            train_p = train_p.append(df)\n",
    "            predictions = predictions.append(df)\n",
    "            actual_data = actual_data.append(df)\n",
    "\n",
    "            X_test_predict = X_test_predict.reset_index(drop = True)\n",
    "            train_p = train_p.reset_index(drop = True)\n",
    "            predictions = predictions.reset_index(drop = True)\n",
    "            actual_data = actual_data.reset_index(drop = True)\n",
    "\n",
    "            series_data_p = time_series_data(X_test_predict, window, lag)\n",
    "\n",
    "            cols_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['Date']]\n",
    "            for i in range(window, 0, -1):\n",
    "                cols_to_drop += [('%s(t-%d)' % (col, i)) for col in ['Date']]\n",
    "\n",
    "            series_data_p.drop(cols_to_drop, axis=1, inplace=True)\n",
    "            series_data_p.drop(['Date(t)'], axis=1, inplace=True)\n",
    "\n",
    "            lbls_col = 'Weekly_Sales(t+%d)' % lag\n",
    "            lbls = series_data_p[lbls_col]\n",
    "            series_data_p = series_data_p.drop(lbls_col, axis=1)\n",
    "\n",
    "            X_test_series_p = series_data_p.values.reshape((series_data_p.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "            subsequences = 2\n",
    "            time_steps_p = X_test_series_p.shape[1] // subsequences\n",
    "            X_test_series_sub_p = X_test_series_p.reshape((X_test_series_p.shape[0], subsequences, time_steps_p, 1))   \n",
    "\n",
    "            cnn_lstm_test_prediction = model_cnn_lstm.predict(X_test_series_sub_p)\n",
    "\n",
    "            train_p.loc[(train_p['Date'] == next_date.strftime(\"%Y-%m-%d\"))  & (train_p['Weekly_Sales'] == 0), ['Weekly_Sales']] = cnn_lstm_test_prediction[0][0]\n",
    "            predictions.loc[(predictions['Date'] == next_date.strftime(\"%Y-%m-%d\")) & (predictions['Weekly_Sales'] == 0), ['Weekly_Sales']] = cnn_lstm_test_prediction[0][0]\n",
    "\n",
    "        graph_data = actual_data.merge(predictions, left_on = ['Date'], right_on = ['Date'], how = 'outer')\n",
    "        graph_data = graph_data.loc[graph_data['Date'] >= '2012-07-29']\n",
    "        graph_data.loc[graph_data['Weekly_Sales_x'] == 0, ['Weekly_Sales_x']] = np.nan #Weekly_Sales_x is actual data and Weekly_Sales_y is the predicted data\n",
    "\n",
    "        plt.subplots(figsize=(15, 6))\n",
    "        plt.subplot(221)\n",
    "        plt.plot(graph_data.Date, graph_data.Weekly_Sales_x, color='blue')\n",
    "        plt.plot(graph_data.Date, graph_data.Weekly_Sales_y, dashes=[10, 5, 10, 5], color='red')\n",
    "        plt.legend(['Actual Sales Data', 'Predicted Sales Data'], loc='upper right')\n",
    "        plt.yticks(np.arange(0, 10, 2))\n",
    "        plt.xticks(graph_data.Date, rotation=90)\n",
    "        plt.title('Weekly sales and forecasts of all stores')\n",
    "        plt.xlabel('Date (yyyy-mm-dd)')\n",
    "        plt.ylabel('Weekly Sales (in 100 million dollars)')\n",
    "        plt.grid(True)\n",
    "    except:\n",
    "        print(\"Insufficient data!\")\n",
    "\n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fig(canvas, figure, loc=(0, 0)):    \n",
    "    fig_canv_agg = FigureCanvasAgg(figure)\n",
    "    fig_canv_agg.draw()\n",
    "    fig_x, fig_y, fig_w, fig_h = figure.bbox.bounds\n",
    "    fig_w, fig_h = int(fig_w), int(fig_h)\n",
    "    img = Tk.PhotoImage(master=canvas, width=fig_w, height=fig_h)\n",
    "\n",
    "    #set position\n",
    "    canvas.create_image(loc[0] + fig_w/2, loc[1] + fig_h/2, image=img)\n",
    "    tkagg.blit(img, fig_canv_agg.get_renderer()._renderer, colormode=2)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Warning - Look and Feel value not valid. Change your ChangeLookAndFeel call. **\n"
     ]
    }
   ],
   "source": [
    "items = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\demand\\\\items.csv', usecols = ['item_code', 'item_name'])\n",
    "item_name = items['item_name'].tolist()\n",
    "\n",
    "stores = pd.read_csv('C:\\\\Users\\\\mrura\\\\Downloads\\\\MRP Final\\\\datasets\\\\demand\\\\stores_geo.csv', usecols = ['store_code'])\n",
    "store_code = stores['store_code'].tolist()\n",
    "\n",
    "sg.ChangeLookAndFeel('White')\n",
    "fig_w, fig_h = 1000, 400\n",
    "\n",
    "col_list = [[sg.Text('Select Store:', font=('current 12'))],\n",
    "            [sg.Listbox(values=store_code, change_submits=True, size=(28, 10), key='store')],\n",
    "            [sg.Text('Select Item:', font=('current 12'))],\n",
    "            [sg.Listbox(values=item_name, change_submits=True, size=(28, 10), key='item')],\n",
    "            [sg.Button('Show Result')]]\n",
    "\n",
    "col_canv = sg.Column([[sg.Canvas(size=(fig_w, fig_h), key='canvas')]])\n",
    "\n",
    "layout = [[sg.Column(col_list), sg.Pane([col_canv], size=(1024, 400))],]\n",
    "\n",
    "window = sg.Window('Demands and Sales forecasts of the company',resizable=True, grab_anywhere=False).Layout(layout)\n",
    "window.Finalize()\n",
    "\n",
    "canv_elm = window.FindElement('canvas')\n",
    "\n",
    "while True:\n",
    "    event, values = window.Read()\n",
    "    \n",
    "    if event in (None, 'Exit'):\n",
    "        break\n",
    "\n",
    "    if event == 'Show Result':\n",
    "        \n",
    "        store = values['store'][0]\n",
    "        store_loc = get_store_loc(store)\n",
    "        if(store == 0):\n",
    "            func = get_result_all\n",
    "            item = 0\n",
    "            item_name = ''\n",
    "        else:\n",
    "            func = get_result_by_item\n",
    "            item = get_item_code(values['item'][0])\n",
    "            item_name = values['item'][0]\n",
    "            if(item == 0):\n",
    "                func = get_result_by_store\n",
    "        \n",
    "        fig = func()\n",
    "        fig_img = draw_fig(canv_elm.TKCanvas, fig)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
